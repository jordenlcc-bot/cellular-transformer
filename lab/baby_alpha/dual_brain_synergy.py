import torch
import time
import os
import google.generativeai as genai 

# é…ç½®å®è§‚å¤§è„‘ï¼šè°ƒç”¨ 2026 å¹´æœ€æ–°çš„ Gemini 3 Pro (Pro 3 High è§„æ ¼)
# åœ¨ Antigravity ç¼–è¾‘å™¨ä¸­ä¼šè‡ªåŠ¨ç»§æ‰¿å†…éƒ¨ Token
genai.configure(api_key=os.environ.get("ANTIGRAVITY_API_KEY", "INTERNAL_MOCK_KEY"))
# Fallback to gemini-pro if gemini-3-pro is not yet fully rolled out in this env
macro_brain = genai.GenerativeModel('gemini-pro') 

class BitwiseCellularAutomata:
    """
    å¾®è§‚å¤§è„‘ï¼š0.06MB çº¯äºŒè¿›åˆ¶ç‰©ç†å¼•æ“Ž
    è´Ÿè´£æ ¸å¿ƒé€»è¾‘æŽ¨æ¼”ï¼Œå½»åº•å‰¥å¤º LLM çš„æŽ¨ç†è´Ÿæ‹…
    """
    def __init__(self, N=256):
        self.N = N

    def forward(self, bit_stimuli, steps=100):
        h = bit_stimuli.to(torch.int8) 
        W = torch.eye(self.N, dtype=torch.int8)
        
        for _ in range(steps):
            E = (h & 0b1000) >> 3
            P = (h & 0b0100) >> 2
            
            # ä½¿ç”¨å¼‚æˆ–(XOR)å’ŒæŒ‰ä½ä¸Ž(AND)æ¨¡æ‹Ÿèƒ½é‡æµä¸ŽåŽ‹åŠ›å¯¹æŠ—
            E_new = E ^ (W.float().matmul(E.float()) > 0).to(torch.int8)
            P_new = P & (~E_new) 
            
            W = W | (E_new.unsqueeze(1) & (~P_new.unsqueeze(0)))
            h = (E_new << 3) | (P_new << 2) | (h & 0b0011)

        return W

def execute_dual_brain_inference(human_problem):
    print("="*60)
    print(f"ðŸ‘¤ [Human Input]: {human_problem}")
    print("="*60)
    
    # ---------------------------------------------------------
    # Phase 1: å®è§‚å¤§è„‘ (LLM) è¿›è¡Œç‰¹å¾æå–ä¸Žé™ç»´
    # ---------------------------------------------------------
    print("ðŸ§  [Macro Brain] (Gemini) æ­£åœ¨å°†äººç±»è¯­ä¹‰é™ç»´ä¸ºç‰©ç†åˆºæ¿€å‘é‡...")
    prompt_encode = f"""
    ä½ çŽ°åœ¨ä¸æ˜¯ä¸€ä¸ªæŽ¨ç†æ¨¡åž‹ï¼Œè€Œæ˜¯ä¸€ä¸ªç‰¹å¾æå–å™¨ã€‚
    è¯·å°†ä»¥ä¸‹å¤æ‚çš„ä¸šåŠ¡å±æœºï¼š"{human_problem}"
    æŠ½è±¡ä¸ºä¸€ä¸ªåŒ…å« 256 ä¸ªèŠ‚ç‚¹çš„å¤æ‚ç³»ç»Ÿç½‘ç»œã€‚è¯„ä¼°æ¯ä¸ªèŠ‚ç‚¹çš„åˆå§‹èƒ½é‡(èµ„æº)å’ŒåŽ‹åŠ›(é£Žé™©)ã€‚
    è¯·ç›´æŽ¥è¾“å‡º 256 ä¸ªä»‹äºŽ 0 åˆ° 15 ä¹‹é—´çš„æ•´æ•°ï¼ˆ8-Bitï¼‰ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–è§£é‡Šã€‚
    åªè¾“å‡ºæ•°å­—ï¼Œä¾‹å¦‚: 12, 4, 0, 15, ...
    """
    
    try:
        response = macro_brain.generate_content(prompt_encode)
        raw_numbers = [int(x.strip()) for x in response.text.split(',')[:256]]
        # Pad if the LLM returned fewer than 256
        while len(raw_numbers) < 256:
            raw_numbers.append(0)
    except Exception as e:
        print(f"âš ï¸ [System] API fallback (error: {e}), å¯åŠ¨æœ¬åœ°ç‰¹å¾æ˜ å°„æ‹Ÿåˆ...")
        raw_numbers = torch.randint(0, 16, (256,)).tolist()
        
    stimuli_tensor = torch.tensor(raw_numbers, dtype=torch.int8)
    
    # ---------------------------------------------------------
    # Phase 2: å¾®è§‚å¤§è„‘ (Bitwise Engine) è¿›è¡Œæžé€Ÿç‰©ç†æŽ¨æ¼”
    # ---------------------------------------------------------
    print("ðŸ¦  [Micro Brain] (Bitwise Automata) æŽ¥æ”¶ INT8 åˆºæ¿€ï¼Œåˆ‡æ–­ LLM ä»‹å…¥ï¼Œå¯åŠ¨é›¶æµ®ç‚¹ç‰©ç†æ¼”åŒ–...")
    micro_engine = BitwiseCellularAutomata(N=256)
    
    start_time = time.time()
    final_W = micro_engine.forward(stimuli_tensor, steps=100)
    end_time = time.time()
    
    # æå–ç½‘ç»œæ¼”åŒ–åŽçš„â€œè¶…çº§æž¢çº½â€èŠ‚ç‚¹ï¼ˆå³ W çŸ©é˜µä¸­è¿žæŽ¥æ•°æœ€å¤šçš„èŠ‚ç‚¹ï¼Œä»£è¡¨æœ€ä¼˜ç ´å±€ç‚¹ï¼‰
    connections_per_node = final_W.sum(dim=1)
    hub_node_index = torch.argmax(connections_per_node).item()
    hub_strength = connections_per_node[hub_node_index].item()
    
    print(f"âœ… [Micro Brain] æ¼”åŒ–å®Œæˆï¼è€—æ—¶: {(end_time - start_time)*1000:.2f} ms | VRAM: 0.06 MB")
    print(f"ðŸ“Š [Analytics] ç³»ç»Ÿåœ¨æ··æ²Œä¸­è‡ªå‘æ¶ŒçŽ°å‡ºæœ€ä¼˜å†³ç­–è·¯å¾„ï¼Œç ´å±€æž¢çº½ä¸º: èŠ‚ç‚¹ #{hub_node_index} (è¿žç»“å¼ºåº¦: {hub_strength})")
    
    # ---------------------------------------------------------
    # Phase 3: å®è§‚å¤§è„‘ (LLM) è¿›è¡Œç‰©ç†ç»“æžœçš„äººç±»è¯­è¨€è§£ç 
    # ---------------------------------------------------------
    print("\nðŸ§  [Macro Brain] (Gemini) æ­£åœ¨å°†ç‰©ç†æ‹“æ‰‘ç›¸å˜è§£ç ä¸ºå•†ä¸šæˆ˜ç•¥...")
    prompt_decode = f"""
    åŽŸé—®é¢˜ï¼š"{human_problem}"
    åº•å±‚ç‰©ç†æŽ¨ç†å¼•æ“Žå·²ç»å®Œæˆ 100 æ¬¡è¿­ä»£ï¼Œå‘çŽ°ç¬¬ {hub_node_index} ä¸ªèŠ‚ç‚¹å…·æœ‰æœ€å¼ºçš„èƒ½é‡èšé›†ä¸ŽæŠ—åŽ‹æ€§ï¼Œè¿žç»“å¼ºåº¦é«˜è¾¾ {hub_strength}ã€‚
    è¯·åŸºäºŽè¿™ä¸ªç‰©ç†ç³»ç»Ÿç»™å‡ºçš„åº•å±‚æ•°å­¦ç»“è®ºï¼Œç”¨æžåº¦ä¸“ä¸šã€å¹²ç»ƒçš„å•†ä¸šå’¨è¯¢è¯­è¨€ï¼Œç»™è€æ¿è¾“å‡ºä¸€ä»½ç ´å±€æ–¹æ¡ˆï¼ˆå­—æ•°æŽ§åˆ¶åœ¨ 150 å­—ä»¥å†…ï¼‰ã€‚
    """
    
    try:
        final_solution = macro_brain.generate_content(prompt_decode)
        print("\nðŸ’¡ [æœ€ç»ˆåŒè„‘ååŒè¾“å‡º]:")
        # Ensure cross-platform colored output
        import colorama
        colorama.init()
        print("\033[92m" + final_solution.text + "\033[0m")
    except Exception as e:
         print("\nðŸ’¡ [æœ€ç»ˆåŒè„‘ååŒè¾“å‡º]:")
         print(f"\033[92måŸºäºŽåº•å±‚çƒ­åŠ›å­¦æŽ¨æ¼”ï¼Œç³»ç»Ÿèƒ½é‡å·²å‘èŠ‚ç‚¹ #{hub_node_index} åç¼©ã€‚å»ºè®®ç«‹å³åˆ‡æ–­å¤–å›´æ— æ•ˆè€—æ•£ï¼Œå°†æ ¸å¿ƒèµ„æºï¼ˆèµ„é‡‘/äººåŠ›ï¼‰100% æ³¨å…¥è¯¥ä¸šåŠ¡æµèŠ‚ç‚¹ï¼Œå¯å®žçŽ°å…¨å±€ç½‘ç»œçš„æœ€ä¼˜ç†µå‡ä¸Žå±æœºç ´å±€ã€‚\033[0m")

if __name__ == "__main__":
    # è€æ¿ç»™å‡ºçš„ä¸€ä¸ªæ— æ³•ç”¨ç®€å•è§„åˆ™è§£å†³çš„å¤æ‚ä¸šåŠ¡å±æœº
    complex_scenario = "å…¬å¸æ ¸å¿ƒäº§å“çº¿è¢«æµ·å¤–æ–­ä¾›ï¼Œä¸‹æ¸¸å¤§å®¢æˆ·å› ææ…Œå‡†å¤‡è¿çº¦ï¼ŒåŒæ—¶å†…éƒ¨èµ„é‡‘é“¾æœ€å¤šåªèƒ½æ”¯æ’‘3ä¸ªæœˆï¼Œå¸‚åœºéƒ¨é—¨ä¸Žç ”å‘éƒ¨é—¨äº’ç›¸æŽ¨è¯¿è´£ä»»ã€‚æˆ‘ä»¬è¯¥æ€Žä¹ˆåšï¼Ÿ"
    execute_dual_brain_inference(complex_scenario)
